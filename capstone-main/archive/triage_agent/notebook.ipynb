{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ENV Variables\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_BASE_URL'] = \"http://localhost:1234/v1\"\n",
    "os.environ['OPENAI_API_KEY'] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install requests\n",
    "# !pip3 install deprecation\n",
    "# !pip3 install langchain\n",
    "# !pip3 install langchain_community\n",
    "# !pip3 install sqlite_vss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Define a class that uses the HTTP API to get embeddings\n",
    "class HTTPEmbeddingModel(Embeddings):\n",
    "    def __init__(self, api_url: str, model_name: str):\n",
    "        \"\"\"\n",
    "        Initialize with the base URL of the HTTP server and model name.\n",
    "        \n",
    "        :param api_url: The API endpoint that returns the embeddings.\n",
    "        :param model_name: The model to use when making the request.\n",
    "        \"\"\"\n",
    "        self.api_url = api_url\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get the embedding for a single piece of text by making an HTTP request.\n",
    "        \n",
    "        :param text: The text to get embeddings for.\n",
    "        :return: A list of floats representing the embedding.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"input\": text\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.api_url, json=payload, headers={\"Content-Type\": \"application/json\"})\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            raise ValueError(f\"Error getting embedding: {response.text}\")\n",
    "        \n",
    "        response_json = response.json()\n",
    "\n",
    "        # Extract the first embedding from the \"data\" field\n",
    "        embedding_data = response_json.get(\"data\", [])\n",
    "        if len(embedding_data) == 0:\n",
    "            raise ValueError(\"No embeddings found in the response.\")\n",
    "\n",
    "        # Assuming we are interested in the first embedding returned\n",
    "        return embedding_data[0].get(\"embedding\", [])\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Embed a list of documents (texts).\n",
    "        \n",
    "        :param texts: List of documents to embed.\n",
    "        :return: A list of lists, where each inner list is an embedding.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            embedding = self.get_embedding(text)\n",
    "            embeddings.append(embedding)\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Embed a single query (text).\n",
    "        \n",
    "        :param text: The query text to embed.\n",
    "        :return: A list of floats representing the embedding.\n",
    "        \"\"\"\n",
    "        return self.get_embedding(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def read_txt_files(folder_path):\n",
    "    all_texts = []\n",
    "    folder_map = {}\n",
    "    # Walk through all subdirectories and files in the folder\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        folder_name = os.path.basename(root)\n",
    "        folder_map[folder_name] = []\n",
    "        for file in files:\n",
    "            # Check if the file has a .txt extension\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                # Read the file content and add it to the list\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    all_texts.append(content)\n",
    "                    folder_map[folder_name].append(content)\n",
    "            else:\n",
    "                print(f\"Skipping non-txt file: {file}\")\n",
    "    \n",
    "    return all_texts, folder_map\n",
    "\n",
    "texts, folder_map = read_txt_files(\"/Users/rugvedsomwanshi/CMU/capstone/archive/chatbot_documents\")\n",
    "print(len(texts))\n",
    "print(len(folder_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import SQLiteVSS\n",
    "from typing import List\n",
    "\n",
    "# Instantiate the HTTP embedding model\n",
    "api_url = \"http://127.0.0.1:1234/v1/embeddings\"\n",
    "model_name = \"nomic-embed-text-v1.5\"\n",
    "embd = HTTPEmbeddingModel(api_url=api_url, model_name=model_name)\n",
    "# api_url = \"https://api.openai.com/v1/embeddings\"  # Use OpenAI's API\n",
    "# model_name = \"text-embedding-ada-002\"  # Use OpenAI's embedding model\n",
    "# embd = HTTPEmbeddingModel(api_url=api_url, model_name=model_name)\n",
    "\n",
    "\n",
    "# Add the documents to the vectorstore using the custom HTTP embedding model\n",
    "# Add the documents to the vectorstore using the custom HTTP embedding model\n",
    "db = SQLiteVSS.from_texts(\n",
    "    texts=texts,      # Extract the text from the document chunks\n",
    "    embedding=embd,   # Use your custom embedding model here\n",
    "    table=\"state_union\",\n",
    "    db_file=\"./test.db\",\n",
    "    metadatas=[{\"file_name\": file_name} for file_name in folder_map.keys()]  # Add file names as metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "search = db.similarity_search_with_score(\"Test\", 5)\n",
    "print(search)\n",
    "print(len(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Router\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def AskQuestionBeforeSS(llm, conversation_history, human_message, current_agent=\"internet_search\",):\n",
    "    system_message = SystemMessage(content=\"\")\n",
    "    conversation_history.append(system_message)\n",
    "    agent_information_message = SystemMessage(content=f\"\")\n",
    "    conversation_history.append(agent_information_message)\n",
    "    conversation_history.append(human_message)\n",
    "    ai_response = llm.invoke(conversation_history)\n",
    "    ai_json_response = json.loads(ai_response.content)\n",
    "    conversation_history.append(ai_response)\n",
    "    if ai_json_response['agent'] == current_agent:\n",
    "        return False\n",
    "    else:\n",
    "        current_agent = ai_json_response['agent']\n",
    "        return True\n",
    "    \n",
    "    \n",
    "current_agent = \"internet_search\"\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"mlx-community/llama-3.2-3b-instruct\", temperature=0)\n",
    "conversation_history = []\n",
    "human_message = HumanMessage(content=\"What is the thing the employee would get if he or she joins the company?\")\n",
    "AskQuestionBeforeSS(llm, conversation_history, human_message, current_agent)\n",
    "print(conversation_history)\n",
    "print(conversation_history[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message = HumanMessage(content=\"What is the weather today?\")\n",
    "AskQuestionBeforeSS(llm, conversation_history, human_message, current_agent)\n",
    "print(conversation_history[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message = HumanMessage(content=\"Where can i get some tacos?\")\n",
    "AskQuestionBeforeSS(llm, conversation_history, human_message, current_agent)\n",
    "print(conversation_history[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message = HumanMessage(content=\"What is the amount of sales in the last quarter?\")\n",
    "AskQuestionBeforeSS(llm, conversation_history, human_message, current_agent)\n",
    "print(conversation_history[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(human_message, db, top_k=1):\n",
    "    search = db.similarity_search_with_score(human_message.content, k=top_k)\n",
    "    return search\n",
    "\n",
    "def AskQuestionAfterSS(llm, conversation_history, ss_agents, human_message, current_agent=\"internet_search\",):\n",
    "    system_message = SystemMessage(content=f'''Determine if the question needs a redirection to another agent or the current agent is capable of answering it. \n",
    "                                   If the current agent is capable of answering it, then proceed with the current agent. We have done a similarity search on the documents and\n",
    "                                   seems like the top agents who have the information are {ss_agents}. \n",
    "                                   Usually, internet_search is not the answer and try to use more of the specialized agents which we have\n",
    "                                   The current agent is {current_agent}. The agents which can answer the question are {ss_agents}\n",
    "                                   The specialized available agents are: internet_search, customer_database_search and organizational_information.\n",
    "                                   ONLY CHOOSE FROM THESE AGENTS. DO NOT CHOOSE FROM ANY OTHER AGENT\n",
    "                                   ''')\n",
    "    conversation_history.append(system_message)\n",
    "    agent_information_message = SystemMessage(content=f\"\")\n",
    "    conversation_history.append(agent_information_message)\n",
    "    conversation_history.append(human_message)\n",
    "    ai_response = llm.invoke(conversation_history)\n",
    "    ai_json_response = json.loads(ai_response.content)\n",
    "    conversation_history.append(ai_response)\n",
    "    if ai_json_response['agent'] == current_agent:\n",
    "        return False\n",
    "    else:\n",
    "        current_agent = ai_json_response['agent']\n",
    "        return True\n",
    "\n",
    "current_agent = \"internet_search\"\n",
    "\n",
    "human_message = HumanMessage(content=\"Does customer satisfication affect employee benefits?\")\n",
    "switched = AskQuestionBeforeSS(llm, conversation_history, human_message, current_agent)\n",
    "\n",
    "if switched:\n",
    "    top_documents = similarity_search(human_message, db, top_k=3)\n",
    "    agents_from_search = set()\n",
    "    for key, values in folder_map.items():\n",
    "        for value in values:\n",
    "            for document in top_documents:\n",
    "                if value == document[0].page_content:\n",
    "                    agents_from_search.add(key)\n",
    "                    print(agents_from_search)\n",
    "    AskQuestionAfterSS(llm, conversation_history, agents_from_search, human_message, current_agent)\n",
    "    print(conversation_history[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadata_search import MetadataSearchEngine\n",
    "\n",
    "# Initialize the search engine\n",
    "search_engine = MetadataSearchEngine(db, folder_map, llm)\n",
    "\n",
    "# Process a query\n",
    "query = \"What are the employee benefits?\"\n",
    "agent, results = search_engine.route_query(query)\n",
    "\n",
    "# Access results with metadata\n",
    "for result in results:\n",
    "    print(f\"Agent: {result.agent}\")\n",
    "    print(f\"Title: {result.metadata['title']}\")\n",
    "    print(f\"Score: {result.score}\")\n",
    "    print(f\"Content: {result.content[:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
